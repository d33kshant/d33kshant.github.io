{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Deekshant Yadav   Data Science Intern </p> <p>  Connect</p> <p>  Contact</p>"},{"location":"#d33kshant","title":"d33kshant","text":"<p>Hey, there! I am Deekshant, a data science intern and a master's student in artificial intelligence. I have a background in computer science and past experience as a full-stack web developer, mainly in backend. I enjoy working with Python, AI-ML and open-source projects. I also like writing about my experiences and sharing what I learn.</p>"},{"location":"#projects","title":"Projects","text":"<p>I prefer project based learning thats why I always try to apply my learning on small or big projects. Over the years I've worked on many projects related to Web, AI and Open Source.</p>"},{"location":"#pinned-projects","title":"Pinned Projects","text":"<ul> <li> <p> \u00a0 Stock Price Prediction with LSTM</p> <p>A Long Short-Term Memory (LSTM) neural network to predict stock prices</p> <p>Open In Google Colab</p> </li> <li> <p> \u00a0 HMM A Minimal Language Model</p> <p>Probability based language modeling to create a very minimal language model</p> <p>View Open In Google Colab</p> </li> <li> <p> \u00a0 Store w/ Recommendation System</p> <p>An online commerece platform with content based recommendation system</p> <p>View Open In GitHub</p> </li> <li> <p> \u00a0 Snake Game Bot</p> <p>A bot that plays classic snake game with reinforcement learning</p> <p>Read Open In Google Colab</p> </li> </ul> <p>Explore More</p>"},{"location":"#blogs","title":"Blogs","text":"<p>I do not write alot but often share my learning, opinions or experience time to time. I use AI to rectify and summaries the content to make it more expressive</p>"},{"location":"#recent-blogs","title":"Recent Blogs","text":"<ul> <li> <p> Introduction to Machine Learning</p> <p> Have you ever wondered how Netflix knows exactly what show you might want to watch next? Or how your email automatically filters spam messages? Or maybe how your phone recognizes your face? All of these technological wonders are powered by machine learning. </p> </li> <li> <p> Implementing Convolutional Neural Networks using Tensorflow</p> <p> Convolutional Neural Networks (CNNs) are a class of deep learning models that excel at working with image data. Instead of processing each pixel independently (like in a fully connected neural network), CNNs use filters (or kernels) to scan across the image, capturing spatial hierarchies and local patterns like edges, textures, and shapes. </p> </li> <li> <p> Implementing Principal Component Analysis (PCA) from Scratch</p> <p> Principal Component Analysis (PCA) is a widely used technique for reducing the dimensionality of datasets while retaining the most important information. It does this by transforming correlated variables into a smaller set of uncorrelated variables called principal components.  <p>Read More</p>"},{"location":"blog/","title":"All Blogs","text":""},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/","title":"Implementing Convolutional Neural Networks using Tensorflow","text":"<p>Convolutional Neural Networks (CNNs) are a class of deep learning models that excel at working with image data. Instead of processing each pixel independently (like in a fully connected neural network), CNNs use filters (or kernels) to scan across the image, capturing spatial hierarchies and local patterns like edges, textures, and shapes.</p> AI Summary <p>This document walks through the implementation of a basic Convolutional Neural Network (CNN) using TensorFlow, designed to classify images from the CIFAR-10 dataset. It introduces core CNN components such as convolutional, pooling, and dense layers, and explains their roles in feature extraction and classification. The model is built with TensorFlow's Keras API, trained for 10 epochs, and achieves around 70% test accuracy. Readers will also learn how to visualize training progress and evaluate model performance. This is a great starting point for beginners to understand how CNNs work and how to implement them from scratch.</p> <p>CNNs consist of layers such as:</p> <ul> <li> <p>Convolutional layers, which apply learnable filters to detect features.</p> </li> <li> <p>ReLU activations, which introduce non-linearity.</p> </li> <li> <p>Pooling layers, which reduce spatial size and help generalize.</p> </li> <li> <p>Fully connected layers, which perform final classification based on extracted features.</p> </li> </ul> <p>This architecture allows CNNs to automatically learn useful features from raw pixel data with minimal pre-processing. They're widely used in image recognition, medical imaging, self-driving cars, and more.</p>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#implementation-of-cnn-using-tensorflow","title":"Implementation of CNN using Tensorflow","text":""},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#1-import-libraries","title":"1. Import Libraries","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#2-load-and-preprocess-the-dataset","title":"2. Load and Preprocess the Dataset","text":"<pre><code>(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n</code></pre> <pre>Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 0us/step\n</pre> <p>The CIFAR-10 dataset contains 60,000 32x32 color images in 10 categories. We normalize the image data and one-hot encode the labels for training.</p>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#3-build-the-cnn-model","title":"3. Build the CNN Model","text":"<pre><code>model = models.Sequential([\n    layers.Input((32, 32, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')  # 10 classes\n], name=\"simple-cnn\")\n\nmodel.summary()\n</code></pre> <pre>Model: \"simple-cnn\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                         \u2503 Output Shape                \u2503         Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 conv2d (Conv2D)                      \u2502 (None, 30, 30, 32)          \u2502             896 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 max_pooling2d (MaxPooling2D)         \u2502 (None, 15, 15, 32)          \u2502               0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 conv2d_1 (Conv2D)                    \u2502 (None, 13, 13, 64)          \u2502          18,496 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 max_pooling2d_1 (MaxPooling2D)       \u2502 (None, 6, 6, 64)            \u2502               0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatten (Flatten)                    \u2502 (None, 2304)                \u2502               0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense (Dense)                        \u2502 (None, 64)                  \u2502         147,520 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                      \u2502 (None, 10)                  \u2502             650 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 167,562 (654.54 KB)\n</pre> <pre> Trainable params: 167,562 (654.54 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <p>This simple CNN has two convolutional layers followed by max-pooling layers. After flattening the features, it passes through two dense layers to produce a probability distribution over the 10 classes.</p> <p>This is a basic convolutional neural network (CNN) that you\u2019ll often see in introductory deep learning tutorials. This model is inspired by LeNet.</p>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#4-compile-and-training-the-model","title":"4.  Compile and Training the Model","text":"<pre><code>model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n</code></pre> <pre><code>history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), )\n</code></pre> <pre>Epoch 1/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 7ms/step - accuracy: 0.3820 - loss: 1.7106 - val_accuracy: 0.5632 - val_loss: 1.2533\nEpoch 2/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 5ms/step - accuracy: 0.5817 - loss: 1.1892 - val_accuracy: 0.6254 - val_loss: 1.0929\nEpoch 3/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 4ms/step - accuracy: 0.6421 - loss: 1.0342 - val_accuracy: 0.6310 - val_loss: 1.0580\nEpoch 4/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 4ms/step - accuracy: 0.6757 - loss: 0.9403 - val_accuracy: 0.6653 - val_loss: 0.9575\nEpoch 5/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 4ms/step - accuracy: 0.7016 - loss: 0.8715 - val_accuracy: 0.6787 - val_loss: 0.9492\nEpoch 6/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 4ms/step - accuracy: 0.7199 - loss: 0.8115 - val_accuracy: 0.6936 - val_loss: 0.8958\nEpoch 7/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 4ms/step - accuracy: 0.7385 - loss: 0.7540 - val_accuracy: 0.6931 - val_loss: 0.9070\nEpoch 8/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 4ms/step - accuracy: 0.7550 - loss: 0.7088 - val_accuracy: 0.6888 - val_loss: 0.9137\nEpoch 9/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 4ms/step - accuracy: 0.7671 - loss: 0.6708 - val_accuracy: 0.7036 - val_loss: 0.8936\nEpoch 10/10\n782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 4ms/step - accuracy: 0.7799 - loss: 0.6332 - val_accuracy: 0.7029 - val_loss: 0.9034\n</pre> <pre><code>plt.plot(history.history['accuracy'], label='train acc')\nplt.plot(history.history['val_accuracy'], label='val acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\nplt.show()\n</code></pre>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#5-evaluate-the-model","title":"5. Evaluate the Model","text":"<pre><code>test_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f'Test accuracy: {test_acc:.4f}')\n</code></pre> <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step - accuracy: 0.7010 - loss: 0.9037\nTest accuracy: 0.7029\n</pre>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#6-sample-prediction","title":"6. Sample Prediction","text":"<pre><code>class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\nindex = np.random.randint(len(x_test))\nsample_image = x_test[index]\ntrue_label = np.argmax(y_test[index])\n\nprediction = model.predict(np.expand_dims(sample_image, axis=0))\npredicted_label = np.argmax(prediction)\n</code></pre> <pre>1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step\n</pre> <pre><code>plt.imshow(sample_image)\nplt.title(f\"Actual: {class_names[true_label]} | Predicted: {class_names[predicted_label]}\")\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"blog/implementing-convolutional-neural-networks-using-tensorflow/#conclusion","title":"Conclusion","text":"<p>We explored what Convolutional Neural Networks (CNNs) are and how to implement a simple one using TensorFlow and Keras. While this model is simple, it's a strong foundation for understanding how more advanced architectures work. CNNs are powerful tools for working with image data because they can automatically learn meaningful patterns and hierarchies from raw pixels.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/","title":"Implementing Decision Tree from Scratch","text":"<p>Decision trees are powerful models that simulate human decision-making by breaking down complex problems into clear, step-by-step choices. They are widely used for classification and regression tasks, providing intuitive and interpretable results.</p> <p></p> AI Summary <p>This blog offers a detailed exploration of decision trees, focusing on their mathematical foundations and practical applications. It explains concepts like Gini impurity, information gain, and data splitting, providing a clear understanding of how decision trees make decisions. Through a step-by-step approach, readers gain insights into building and evaluating decision trees. The blog concludes with a comparison to a standard implementation, reinforcing the learning experience.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#what-is-decision-tree","title":"What is Decision Tree","text":"<p>A decision tree is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-like structure where each internal node represents a decision based on a specific feature, each branch represents the outcome of that decision, and each leaf node provides the final prediction.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#understanding-decision-tree","title":"Understanding Decision Tree","text":"<p>The core idea behind decision trees is to minimize impurity when making splits. The impurity of a node can be measured using metrics like:</p> <ol> <li> <p>Gini Impurity:  measure of how mixed or impure a dataset is, commonly used in decision tree algorithms to determine the best way to split data for classification</p> \\[ Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2 \\] <p>where \\(p_i\\) is the probability of class \\(i\\) in the dataset \\(D\\).</p> </li> <li> <p>Entropy: Entropy measures the disorder or randomness in a dataset. It's calculated by summing the weighted probabilities of each possible outcome, using the logarithm of the probabilities</p> \\[ H(D) = - \\sum_{i=1}^{C} p_i \\log_2 p_i \\] <p>The goal is to select the split that results in the lowest weighted impurity of the child nodes.</p> </li> <li> <p>Information Gain: Information gain measures the reduction in entropy (or increase in purity) achieved by splitting a dataset on a particular feature, The information gain for a split is calculated as:</p> \\[ IG = H(D) - \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} H(D_i) \\] <p>where \\(D_i\\) represents the subsets resulting from the split.</p> </li> </ol>"},{"location":"blog/implementing-decision-tree-from-scratch/#implementation-of-decision-tree","title":"Implementation of Decision Tree","text":""},{"location":"blog/implementing-decision-tree-from-scratch/#1-import-libraries-and-load-the-dataset","title":"1. Import Libraries and Load the Dataset","text":"<pre><code>iris = datasets.load_iris()\ndata = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\ndata.head()\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 <p>This block imports the necessary libraries, loads the Iris dataset using <code>sklearn.datasets</code>, and converts it into a pandas DataFrame for easier manipulation.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#2-calculate-gini-impurity","title":"2. Calculate Gini Impurity","text":"<pre><code>def gini_impurity(y):\n    classes, counts = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    return 1 - np.sum(probabilities ** 2)\n</code></pre> <p>This function calculates the Gini impurity of a given set of labels. It is used to measure how impure a node is, with 0 indicating perfect purity.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#3-perform-data-splitting","title":"3. Perform Data Splitting","text":"<pre><code>def split_data(X, y, feature_index, threshold):\n    left_mask = X[:, feature_index] &lt;= threshold\n    right_mask = ~left_mask\n    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n</code></pre> <p>This function splits the dataset based on a specified feature and threshold. It returns the left and right subsets of features and labels.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#4-calculate-information-gain","title":"4. Calculate Information Gain","text":"<pre><code>def information_gain(X, y, feature_index, threshold):\n    parent_impurity = gini_impurity(y)\n    X_left, X_right, y_left, y_right = split_data(X, y, feature_index, threshold)\n    n = len(y)\n    left_weight = len(y_left) / n\n    right_weight = len(y_right) / n\n    gain = parent_impurity - (left_weight * gini_impurity(y_left) + right_weight * gini_impurity(y_right))\n    return gain\n</code></pre> <p>This function calculates the information gain from splitting the data using a specific feature and threshold.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#5-find-the-best-split","title":"5. Find the Best Split","text":"<pre><code>def find_best_split(X, y):\n    best_gain = 0\n    best_feature = None\n    best_threshold = None\n    for feature_index in range(X.shape[1]):\n        thresholds = np.unique(X[:, feature_index])\n        for threshold in thresholds:\n            gain = information_gain(X, y, feature_index, threshold)\n            if gain &gt; best_gain:\n                best_gain = gain\n                best_feature = feature_index\n                best_threshold = threshold\n    return best_feature, best_threshold\n</code></pre> <p>This function iterates over all features and possible thresholds to find the best split with the highest information gain.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#6-build-the-decision-tree-recursively","title":"6. Build the Decision Tree Recursively","text":"<pre><code>def build_tree(X, y, depth=0, max_depth=5):\n    if len(np.unique(y)) == 1 or depth == max_depth:\n        return np.argmax(np.bincount(y.astype(int)))\n\n    feature_index, threshold = find_best_split(X, y)\n    if feature_index is None:\n        return np.argmax(np.bincount(y.astype(int)))\n\n    X_left, X_right, y_left, y_right = split_data(X, y, feature_index, threshold)\n    left_subtree = build_tree(X_left, y_left, depth + 1, max_depth)\n    right_subtree = build_tree(X_right, y_right, depth + 1, max_depth)\n\n    return (feature_index, threshold, left_subtree, right_subtree)\n</code></pre> <p>This recursive function builds the decision tree by selecting the best split, creating subtrees, and returning the tree structure as a tuple.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#7-make-predictions","title":"7. Make Predictions","text":"<pre><code>def predict(sample, tree):\n    if isinstance(tree, int):\n        return tree\n    feature_index, threshold, left_subtree, right_subtree = tree\n    if sample[feature_index] &lt;= threshold:\n        return predict(sample, left_subtree)\n    else:\n        return predict(sample, right_subtree)\n</code></pre> <p>This function traverses the tree for a given input sample and returns the predicted class.</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#8-running-and-evaluating-the-model","title":"8. Running and Evaluating The Model","text":"<pre><code>X = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\ntree = build_tree(X, y)\npredictions = [predict(sample, tree) for sample in X]\naccuracy = np.sum(predictions == y) / len(y)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n</code></pre> <p>Accuracy: 100.00%</p>"},{"location":"blog/implementing-decision-tree-from-scratch/#using-scikit-learn-for-decision-tree","title":"Using Scikit-learn for Decision Tree","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\n\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\nclf = DecisionTreeClassifier()\nclf.fit(X, y)\n\ny_pred = clf.predict(X)\nprint(f\"Accuracy: {accuracy_score(y, y_pred) * 100:.2f}%\")\n\nplt.figure(figsize=(15, 10))\nplot_tree(clf, filled=True)\nplt.show()\n</code></pre> <p>Accuracy: 100.00% </p> <p>Here <code>x[2]</code> is the petal length, <code>x[3]</code> is the petal width, orange color represents <code>setosa</code>, green represents <code>versicolor</code>, and purple represents <code>virginica</code></p>"},{"location":"blog/implementing-decision-tree-from-scratch/#conclusion","title":"Conclusion","text":"<p>In this blog, we learned how decision trees work by implementing one from scratch using NumPy and pandas in a functional, procedural style. We calculated Gini impurity, found the best split using information gain, and recursively built a tree. After that, we compared our implementation with the <code>scikit-learn</code> decision tree classifier. This exercise helps in building a solid understanding of decision trees, which are a foundation for more advanced algorithms like random forests and gradient boosting.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/","title":"Implementing K Means Clustering From Scratch","text":"<p>K-Means clustering is one of the simplest and most effective unsupervised machine learning algorithms. It is widely used for tasks such as customer segmentation and anomaly detection</p> <p></p> AI Summary <p>This blog provides a comprehensive tutorial on K-Means clustering, an unsupervised machine learning algorithm used for grouping data into clusters. It explains the algorithm's steps, including initialization, assignment, and update phases, and demonstrates a from-scratch implementation using NumPy and Pandas on the Iris dataset. The tutorial covers both manual implementation and Scikit-Learn's approach, showcasing how the algorithm works through code, mathematical formulas, and visualization of clustering results.</p> <p>In this blog, we'll break down the concept of K-Means clustering, implement it from scratch using NumPy and Pandas, and apply it to the famous Iris dataset. We'll follow a procedural, function-based approach, just like you would in a Jupyter notebook. Finally, we'll see how to implement the same using Scikit-Learn.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#what-is-k-means-clustering","title":"What is K-Means Clustering?","text":"<p>K-Means clustering is an unsupervised learning algorithm used to group data into \\( k \\) clusters. The goal is to minimize the variance within each cluster, ensuring data points in the same cluster are as similar as possible.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#steps-of-k-means-algorithm","title":"Steps of K-Means Algorithm","text":"<ol> <li>Initialization: Choose \\( k \\) initial cluster centroids randomly.</li> <li>Assignment: Assign each data point to the nearest centroid using the Euclidean distance formula:</li> </ol> \\[ \\text{Distance} = \\sqrt{\\sum_{i=1}^{n}(x_i - \\mu_i)^2} \\] <ol> <li>Update: Calculate new centroids as the mean of all points in a cluster.</li> </ol> \\[ \\mu_k = \\frac{1}{N_k} \\sum_{i=1}^{N_k} x_i \\] <ol> <li>Repeat: Perform the assignment and update steps until centroids no longer change or a maximum number of iterations is reached.</li> </ol>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#implementation-k-means-clustering","title":"Implementation K Means Clustering","text":""},{"location":"blog/implementing-k-means-clustering-from-scratch/#1-importing-libraries","title":"1. Importing Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#2-preparing-the-dataset","title":"2. Preparing the Dataset","text":"<p><pre><code>iris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\ntrue_labels = iris.target\ndata.head()\n</code></pre> This loads the Iris dataset and converts it into a Pandas DataFrame for easier manipulation.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#3-building-the-model","title":"3. Building the Model","text":""},{"location":"blog/implementing-k-means-clustering-from-scratch/#some-helper-functions","title":"Some Helper Functions","text":"<p><pre><code>def distance(here, there):\n    return np.sqrt(np.sum((here - there) ** 2))\n</code></pre> This function calculates the Euclidean distance between two points.</p> <p><pre><code>def assign_clusters(data, centroids):\n    labels = []\n    for point in data:\n        distances = [distance(point, centroid) for centroid in centroids]\n        labels.append(np.argmin(distances))\n    return np.array(labels)\n</code></pre> We assign each point to the nearest cluster based on the calculated distances.</p> <p><pre><code>def update_centroids(data, labels, k):\n    return np.array([data[labels == i].mean(axis=0) for i in range(k)])\n</code></pre> Centroids are updated by taking the mean of all points in each cluster.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#k-means-clustering","title":"K-Means Clustering","text":"<p><pre><code>def kmeans(data, k, max_iters=100, tol=1e-4):\n    centroids = data.sample(n=k).to_numpy()\n    data = data.to_numpy()\n    for _ in range(max_iters):\n        old_centroids = centroids.copy()\n        labels = assign_clusters(data, centroids)\n        centroids = update_centroids(data, labels, k)\n        if np.linalg.norm(centroids - old_centroids) &lt; tol:\n            break\n    return labels, centroids\n</code></pre> This function brings everything together to apply K-Means clustering.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#forming-the-clusters","title":"Forming the Clusters","text":"<p><pre><code>labels, centroids = kmeans(data.iloc[:, [2, 3]], k=3)\n</code></pre> Here we have only used feature at index <code>2</code> and <code>3</code> which are Petal Width and Petal Length.</p> <pre><code>print(f\"Silhouette Score: {silhouette_score(data.iloc[:, [2, 3]], labels):.2f}\")\n</code></pre> <p>Silhouette Score: 0.66</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#4-visualizing-results","title":"4. Visualizing Results","text":"<pre><code># Plotting actual classes\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(data.iloc[:, 2], data.iloc[:, 3], c=iris.target, cmap='brg', zorder=2)\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.title('Actual Classes')\nplt.grid()\n\n# Plotting predicted clusters\nplt.subplot(1, 2, 2)\nplt.scatter(data.iloc[:, 2], data.iloc[:, 3], c=labels, cmap='winter', zorder=2)\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.title('K-Means Clustering Results')\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>This plots both the actual classes and the predicted clusters side by side for comparison.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#k-means-using-scikit-learn","title":"K-Means using Scikit-Learn","text":"<p><pre><code>from sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters=3, random_state=42)\nkmeans_model.fit(data)\npreds = kmeans_model.predict(data)\n</code></pre> With just a few lines of code, Scikit-Learn simplifies K-Means clustering.</p>"},{"location":"blog/implementing-k-means-clustering-from-scratch/#conclusion","title":"Conclusion","text":"<p>K-Means clustering is a fundamental unsupervised learning algorithm. Implementing it from scratch provides valuable insight into its inner workings. By following a step-by-step approach, you can better understand how data points are grouped, how centroids are updated, and how clusters are formed.</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/","title":"Implementing Logistic Regression from Scratch","text":"<p>Let's dive into logistic regression, a fundamental classification algorithm. We'll implement it from scratch, break down the code step by step, and demonstrate its application on a popular dataset.</p> <p></p> AI Summary <p>The document provides a comprehensive tutorial on implementing logistic regression from scratch, a fundamental classification algorithm used for binary prediction tasks. It explains the mathematical foundations of logistic regression, including the sigmoid function and decision boundary, and demonstrates a step-by-step implementation using NumPy on the Breast Cancer dataset. The tutorial covers data preparation, model training, prediction, and evaluation, comparing the manual implementation with Scikit-Learn's approach and achieving high accuracy in classifying cancer data.</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#what-is-logistic-regression","title":"What is Logistic Regression?","text":"<p>Logistic Regression is a linear model used for classification. It applies a logistic (sigmoid) function to the linear combination of input features to predict a probability between 0 and 1.</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#sigmoid-function","title":"Sigmoid Function","text":"<p>The sigmoid function is defined as follows:</p> \\[ sigmoid(z) = \\frac{1}{1 + e^{-z}} \\] <p>Here \\( z = w^T x + b \\) is the linear combination of weights and inputs. \\( w \\) are the model weights, \\( b \\) is the bias.</p> <p></p> <p>Sigmoid Curve</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#decision-boundary","title":"Decision Boundary","text":"<p>The model predicts a class based on a threshold, typically 0.5:</p> \\[ \\hat{y} = \\begin{cases} 1 &amp; \\text{if } \\sigma(z) \\ge 0.5 \\\\ 0 &amp; \\text{if } \\sigma(z) &lt; 0.5 \\end{cases} \\]"},{"location":"blog/implementing-logistic-regression-from-scratch/#implementation-of-logistic-regression","title":"Implementation of Logistic Regression","text":""},{"location":"blog/implementing-logistic-regression-from-scratch/#1-importing-the-libraries","title":"1. Importing the Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"blog/implementing-logistic-regression-from-scratch/#2-preparing-the-dataset","title":"2. Preparing the Dataset","text":"<p><pre><code>data = datasets.load_breast_cancer()\nX = data.data\ny = data.target\n\n# Normalize the features\nX = (X - X.mean(axis=0)) / X.std(axis=0)\n\n# Add intercept term\nX = np.c_[np.ones((X.shape[0], 1)), X]\n</code></pre> We load the Breast Cancer dataset. Normalize the features to ensure they are on a similar scale. Add an intercept (bias) term with a column of ones.</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#3-building-and-training-of-the-model","title":"3. Building and Training of the Model","text":"<p><pre><code>def sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n</code></pre> Above code is the implementation of the sigmoid function.</p> <pre><code>def logistic_regression(X, y, alpha=0.01, epochs=100):\n    m = len(y)\n    w, b = np.zeros(X.shape[1]), 0\n    for _ in range(epochs):\n        z = np.dot(X, w) + b\n        predictions = sigmoid(z)\n        w -= alpha * np.dot(X.T, (predictions - y)) / m\n        b -= alpha * np.mean(predictions - y)\n    return w, b\n</code></pre> <p>The alogrithm used for the learning is Gradient Descent </p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#4-making-predictions-and-evaluating-the-model","title":"4. Making Predictions and Evaluating the Model","text":"<pre><code>def predict(X, w, b):\n    return (sigmoid(np.dot(X, w) + b) &gt;= 0.5).astype(int)\n\nw, b = logistic_regression(X, y)\ny_pred = predict(X, w, b)\naccuracy = np.mean(y_pred == y)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n</code></pre> <p>Accuracy: 94.55%</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#visualizing-the-results","title":"Visualizing the Results","text":"<pre><code>plt.scatter(X[:, 1], y, zorder=2)\nplt.scatter(X[:, 1], sigmoid(np.dot(X, w) + b), zorder=2)\nplt.grid()\nplt.show()\n</code></pre>"},{"location":"blog/implementing-logistic-regression-from-scratch/#implementing-logistic-regression-using-scikit-learn","title":"Implementing Logistic Regression using Scikit-Learn","text":"<pre><code>from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X[:, 1:], y)\nsklearn_accuracy = model.score(X[:, 1:], y)\nprint(f\"Sklearn Accuracy: {sklearn_accuracy * 100:.2f}%\")\n</code></pre> <p>Sklearn Accuracy: 98.77%</p>"},{"location":"blog/implementing-logistic-regression-from-scratch/#conclusion","title":"Conclusion","text":"<p>Logistic Regression is a powerful yet simple algorithm for binary classification tasks. Implementing it from scratch helps in understanding the core concepts of model building, optimization, and evaluation. By leveraging libraries like NumPy and pandas, we can gain a deeper insight into how logistic regression works under the hood.</p>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/","title":"Implementing Principal Component Analysis (PCA) from Scratch","text":"<p>Principal Component Analysis (PCA) is a widely used technique for reducing the dimensionality of datasets while retaining the most important information. It does this by transforming correlated variables into a smaller set of uncorrelated variables called principal components.</p> <p></p> AI Summary <p>This document explains Principal Component Analysis (PCA), a dimensionality reduction technique that transforms correlated variables into a smaller set of uncorrelated principal components. It provides a step-by-step implementation from scratch using NumPy and Pandas on the Iris dataset, covering data standardization, covariance matrix computation, eigenvalue/eigenvector calculation, and data projection. The tutorial includes visualizations of the transformed data, compares the manual implementation with Scikit-Learn's approach, and emphasizes PCA's value for feature selection, noise reduction, and data visualization in machine learning workflows.</p> <p>PCA is useful in scenarios where we have high-dimensional data and want to reduce complexity, remove noise, and visualize patterns more easily. Some common applications include image compression, feature extraction, and speeding up machine learning models.</p>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#how-pca-works","title":"How PCA Works","text":"<p>PCA follows these key steps:</p> <ol> <li>Standardization: Ensure that all features contribute equally by normalizing the dataset.</li> <li>Compute Covariance Matrix: Measure how features vary together.</li> <li>Compute Eigenvalues and Eigenvectors: Identify principal components in the data.</li> <li>Sort and Select Principal Components: Choose components with the highest variance.</li> <li>Transform Data: Project the original data onto the new feature space.</li> </ol> <p>Mathematically, given a dataset \\(X\\) of shape \\(m \\times n\\), where \\(m\\) is the number of samples and \\(n\\) is the number of features, PCA finds a transformation matrix \\(W\\) such that:</p> \\[ Z = X W \\] <p>where \\(Z\\) represents the data in the new reduced-dimensional space.</p> <p>Now, let's implement PCA from scratch using NumPy and Pandas.</p>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#implementing-pca-from-scratch","title":"Implementing PCA from Scratch","text":""},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#1-importing-necessary-libraries","title":"1. Importing Necessary Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#2-preparing-the-dataset","title":"2. Preparing the Dataset","text":"<p>We will use the famous Iris dataset, which contains four features.</p> <pre><code>dataset = load_iris()\nX = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ny = dataset.target\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#3-standardizing-the-data","title":"3. Standardizing the Data","text":"<p>Since PCA is affected by scale, we standardize the dataset to have mean \\(0\\) and variance \\(1\\).</p> <pre><code>X_std = (X - X.mean()) / X.std()\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#4-computing-the-covariance-matrix","title":"4. Computing the Covariance Matrix","text":"<p>The covariance matrix captures relationships between features.</p> <pre><code>cov_matrix = np.cov(X.T)\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#5-computing-eigenvalues-and-eigenvectors","title":"5. Computing Eigenvalues and Eigenvectors","text":"<p>Eigenvalues represent the variance explained by each component, and eigenvectors define the principal components.</p> <pre><code>eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#6-sorting-eigenvectors-by-eigenvalues","title":"6. Sorting Eigenvectors by Eigenvalues","text":"<p>We sort components by the amount of variance they capture.</p> <pre><code>sorted_indices = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[sorted_indices]\neigenvectors = eigenvectors[:, sorted_indices]\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#7-selecting-the-top-principal-components","title":"7. Selecting the Top Principal Components","text":"<p>We select the top \\(k\\) components. Here, we choose \\(k=2\\) for visualization.</p> <pre><code>k = 2\nW = eigenvectors[:, :k]\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#8-projecting-data-onto-new-feature-space","title":"8. Projecting Data onto New Feature Space","text":"<p>Finally, we transform our dataset using the selected principal components.</p> <pre><code>X_pca = np.dot(X_std, W)\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#9-visualizing-the-results","title":"9. Visualizing the Results","text":"<pre><code>plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', zorder=2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Projection of Iris Dataset')\nplt.grid()\nplt.show()\n</code></pre>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#implementing-pca-using-scikit-learn","title":"Implementing PCA Using Scikit-Learn","text":"<p>While implementing PCA from scratch provides a deeper understanding, real-world applications often use Scikit-Learn for efficiency.</p> <pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca_sklearn = pca.fit_transform(X_std)\n\nplt.scatter(X_pca_sklearn[:, 0], X_pca_sklearn[:, 1], c=y, cmap='viridis', zorder=2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA with Scikit-Learn')\nplt.grid()\nplt.show()\n</code></pre> <p></p>"},{"location":"blog/implementing-principal-component-analysis-pca-from-scratch/#conclusion","title":"Conclusion","text":"<p>PCA is a powerful dimensionality reduction technique that helps in feature selection, noise reduction, and data visualization. By implementing PCA from scratch, we gain insights into its inner workings. However, in practical scenarios, using libraries like Scikit-Learn makes it easier to apply PCA efficiently. Whether from scratch or using libraries, PCA remains a fundamental tool in data science and machine learning.</p>"},{"location":"blog/implementing-q-learning-from-scratch/","title":"Implementing Q Learning from Scratch","text":"<p>Have you ever wondered how an AI agent can learn to play a game\u2014like Snake, Pac-Man, or even chess\u2014just by trial and error? Behind the scenes, a powerful concept called Q-learning is often at work. Q-learning is a model-free reinforcement learning algorithm. That means the agent doesn\u2019t need to know the rules of the environment\u2014it learns them by experience.</p> <p></p> AI Summary <p>This blog introduces Q-learning, a reinforcement learning algorithm that enables agents to learn optimal actions through trial and error. It covers the key components\u2014states, actions, rewards, and the Q-table\u2014and explains how the agent updates its knowledge using the Bellman equation. With a balance of exploration and exploitation, Q-learning helps agents improve their behavior over time without needing a model of the environment.</p>"},{"location":"blog/implementing-q-learning-from-scratch/#what-is-q-learning","title":"What is Q-Learning?","text":"<p>Q-learning is a type of reinforcement learning algorithm that helps an agent learn what action to take in each state to maximize its total reward\u2014purely through trial and error. It doesn\u2019t require a model of the environment, making it model-free.</p>"},{"location":"blog/implementing-q-learning-from-scratch/#key-components-in-q-learning","title":"Key Components in Q-Learning","text":"<ul> <li> <p>Agent: The decision-maker (e.g., a snake in the Snake game).</p> </li> <li> <p>Environment: The world the agent interacts with (e.g., a grid).</p> </li> <li> <p>State \\( s \\): A snapshot of the environment.</p> </li> <li> <p>Action \\( a \\): A move the agent can take.</p> </li> <li> <p>Reward \\( r \\): Feedback received after an action.</p> </li> <li> <p>Q-table \\( Q(s, a) \\): Stores the expected future rewards for each action in each state.  </p> </li> </ul>"},{"location":"blog/implementing-q-learning-from-scratch/#how-q-learning-works","title":"How Q-Learning Works","text":"<p>The agent learns by updating Q-values using the Bellman equation:</p> \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] \\] <p>Where:</p> <ul> <li> <p>\\( \\alpha \\): Learning rate (how quickly it learns)</p> </li> <li> <p>\\( \\gamma \\): Discount factor (importance of future rewards)</p> </li> <li> <p>\\( r \\): Immediate reward</p> </li> <li> <p>\\( s' \\): New state after taking action \\( a \\)</p> </li> <li> <p>\\( \\max_{a'} Q(s', a') \\): Best possible future reward from next state.  </p> </li> </ul> <p>Over time, the agent uses an explore-exploit strategy to balance learning new actions (exploration) and choosing the best-known actions (exploitation), improving the Q-table until it converges to optimal behavior.</p>"},{"location":"blog/implementing-q-learning-from-scratch/#implementation-of-q-learning","title":"Implementation of Q-Learning","text":""},{"location":"blog/implementing-q-learning-from-scratch/#1-importing-the-libraries","title":"1. Importing the Libraries","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport random\nfrom collections import defaultdict\n</code></pre>"},{"location":"blog/implementing-q-learning-from-scratch/#2-declare-constant-values","title":"2. Declare Constant Values","text":"<pre><code># Constants\nGRID_SIZE = 10\nEPISODES = 500\nMAX_STEPS = 100\nEPSILON_DECAY = 0.995\nMIN_EPSILON = 0.01\nALPHA = 0.1\nGAMMA = 0.9\n\n# Directions\nUP = 0\nDOWN = 1\nLEFT = 2\nRIGHT = 3\nDIRECTION_VECTORS = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n\n# Colors\nRED = [255, 0, 0]\nGREEN = [0, 255, 0]\n</code></pre>"},{"location":"blog/implementing-q-learning-from-scratch/#3-creating-the-snakes-environment","title":"3. Creating the Snake's Environment","text":"<pre><code>class SnakeEnv:\n    def __init__(self, size=GRID_SIZE):\n        self.size = size\n        self.reset()\n\n    def reset(self):\n        self.snake = [(self.size // 2, self.size // 2)]\n        self.direction = random.choice([UP, DOWN, LEFT, RIGHT])\n        self.place_food()\n        self.done = False\n        return self.get_state()\n\n    def place_food(self):\n        while True:\n            self.food = (random.randint(0, self.size - 1), random.randint(0, self.size - 1))\n            if self.food not in self.snake:\n                break\n\n    def get_state(self):\n        head = self.snake[0]\n        dir_vector = DIRECTION_VECTORS[self.direction]\n        food_dir = (np.sign(self.food[0] - head[0]), np.sign(self.food[1] - head[1]))\n        danger = self.check_danger()\n        return (dir_vector, food_dir, danger)\n\n    def check_danger(self):\n        head = self.snake[0]\n        danger = []\n        for d in range(4):\n            dx, dy = DIRECTION_VECTORS[d]\n            nx, ny = head[0] + dx, head[1] + dy\n            if (nx &lt; 0 or nx &gt;= self.size or ny &lt; 0 or ny &gt;= self.size or (nx, ny) in self.snake):\n                danger.append(1)\n            else:\n                danger.append(0)\n        return tuple(danger)\n\n    def step(self, action):\n        if self.done:\n            return self.get_state(), 0, self.done\n\n        self.direction = action\n        dx, dy = DIRECTION_VECTORS[self.direction]\n        head = self.snake[0]\n        new_head = (head[0] + dx, head[1] + dy)\n\n        if (new_head in self.snake or \n            not (0 &lt;= new_head[0] &lt; self.size) or \n            not (0 &lt;= new_head[1] &lt; self.size)):\n            self.done = True\n            return self.get_state(), -10, True\n\n        self.snake.insert(0, new_head)\n\n        if new_head == self.food:\n            self.place_food()\n            reward = 10\n        else:\n            self.snake.pop()\n            reward = -0.1\n\n        return self.get_state(), reward, self.done\n\n    def render(self):\n        grid = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n        grid[:, :] = 255\n        for (x, y) in self.snake:\n            grid[x, y] = RED\n        fx, fy = self.food\n        grid[fx, fy] = GREEN\n        return grid\n</code></pre>"},{"location":"blog/implementing-q-learning-from-scratch/#4-the-q-learning-agent","title":"4. The Q-Learning Agent","text":"<pre><code>class QLearningAgent:\n    def __init__(self, actions):\n        self.q_table = defaultdict(lambda: np.zeros(len(actions)))\n        self.actions = actions\n        self.epsilon = 1.0\n\n    def get_action(self, state):\n        state_key = str(state)\n        if np.random.rand() &lt; self.epsilon:\n            return random.choice(self.actions)\n        else:\n            return int(np.argmax(self.q_table[state_key]))\n\n    def learn(self, state, action, reward, next_state):\n        state_key = str(state)\n        next_state_key = str(next_state)\n        predict = self.q_table[state_key][action]\n        target = reward + GAMMA * np.max(self.q_table[next_state_key])\n        self.q_table[state_key][action] += ALPHA * (target - predict)\n</code></pre>"},{"location":"blog/implementing-q-learning-from-scratch/#5-training-the-agent-in-the-environment","title":"5. Training the Agent In The Environment","text":"<pre><code>env = SnakeEnv()\nagent = QLearningAgent(actions=[UP, DOWN, LEFT, RIGHT])\n\nfor episode in range(EPISODES):\n    state = env.reset()\n    total_reward = 0\n    for _ in range(MAX_STEPS):\n        action = agent.get_action(state)\n        next_state, reward, done = env.step(action)\n        agent.learn(state, action, reward, next_state)\n        state = next_state\n        total_reward += reward\n        if done:\n            break\n    agent.epsilon = max(MIN_EPSILON, agent.epsilon * EPSILON_DECAY)\n    if (episode + 1) % 100 == 0:\n        print(f\"Episode {episode+1}, Total reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}\")\n</code></pre> <pre>\nEpisode 100, Total reward: -14.200, Epsilon: 0.606\nEpisode 200, Total reward: -11.500, Epsilon: 0.367\nEpisode 300, Total reward: 8.100, Epsilon: 0.222\nEpisode 400, Total reward: -0.600, Epsilon: 0.135\nEpisode 500, Total reward: 16.900, Epsilon: 0.082\n</pre>"},{"location":"blog/implementing-q-learning-from-scratch/#6-visualizing-the-training","title":"6. Visualizing The Training","text":"<pre><code>frames = []\nstate = env.reset()\n\nfig = plt.figure(figsize=(5, 5))\nplt.axis(\"off\")\n\nfor _ in range(100):\n    grid = env.render()\n    im = plt.imshow(grid, animated=True)\n    frames.append([im])\n    action = agent.get_action(state)\n    state, _, done = env.step(action)\n    if done:\n        break\n\nani = animation.ArtistAnimation(fig, frames, interval=200, blit=True)\nplt.close()\nHTML(ani.to_jshtml())\n</code></pre>"},{"location":"blog/implementing-q-learning-from-scratch/#conclusion","title":"Conclusion","text":"<p>Q-learning is a simple yet powerful way for agents to learn optimal behavior by interacting with their environment. With just a table of values and a smart update rule, it allows agents to improve over time\u2014learning entirely from rewards, not instructions. It's a foundational technique in reinforcement learning and a great starting point for building intelligent systems.</p>"},{"location":"blog/implementing-svm-from-scratch/","title":"Implementing SVM from Scratch","text":"<p>Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes while maximizing the margin between them. SVM is especially effective in high-dimensional spaces and robust to outliers.</p> <p></p> AI Summary <p>In this blog, we explored the fundamentals of Support Vector Machines, starting from the theory behind how they work to a detailed step-by-step implementation using only NumPy and pandas. We built a linear SVM from scratch without relying on object-oriented programming, applied it to a real dataset, and visualized the decision boundaries to better understand the model's behavior. We then demonstrated how easily the same task can be accomplished using scikit-learn. This hands-on approach not only reinforces the theoretical concepts but also provides practical insights into building machine learning models from the ground up.</p>"},{"location":"blog/implementing-svm-from-scratch/#what-is-support-vector-machine","title":"What is Support Vector Machine?","text":"<p>Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. It is especially effective for high-dimensional spaces and situations where the number of dimensions exceeds the number of samples.</p> <p>The core idea behind SVM is to find the hyperplane that best separates the data points of different classes. The optimal hyperplane is the one that maximizes the margin, which is the distance between the hyperplane and the nearest data point of each class. These nearest points are called support vectors.</p>"},{"location":"blog/implementing-svm-from-scratch/#how-does-svm-work","title":"How Does SVM Work?","text":"<p>Mathematically, given a labeled training dataset \\((x_i, y_i)\\) where \\(x_i \\in \\mathbb{R}^n\\) and \\(y_i \\in \\{-1, 1\\}\\), the objective of the linear SVM is to find a weight vector \\(w\\) and bias \\(b\\) such that the decision function:</p> \\[ f(x) = w^T x + b \\] <p>correctly classifies the data while maximizing the margin.</p> <p>The optimization problem becomes:</p> \\[ \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\\\ \\text{ subject to } y_i (w^T x_i + b) \\geq 1 \\] <p>This is a convex quadratic optimization problem, and we can solve it using gradient descent on a hinge loss function with regularization:</p> \\[ \\text{Loss} = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i} \\max(0, 1 - y_i (w^T x_i + b)) \\]"},{"location":"blog/implementing-svm-from-scratch/#implementation-of-support-vector-machine","title":"Implementation of Support Vector Machine","text":""},{"location":"blog/implementing-svm-from-scratch/#1-importing-the-libraries","title":"1. Importing the Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n</code></pre> <p>We import essential libraries: NumPy for numerical computation, pandas for data manipulation, and matplotlib for visualization. We also use <code>make_classification</code> to create a synthetic dataset.</p>"},{"location":"blog/implementing-svm-from-scratch/#2-preparing-the-dataset","title":"2. Preparing the Dataset","text":"<pre><code>X, y = make_classification(\n    n_samples=500, n_features=2,\n    n_redundant=0, n_informative=2,\n    n_clusters_per_class=1,\n)\n\n# Convert labels from {0, 1} to {-1, 1}\ny = np.where(y == 0, -1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n)\n</code></pre> <p>We generate a simple 2D binary classification dataset. The labels are converted to {-1, 1} as required by the SVM formulation.</p>"},{"location":"blog/implementing-svm-from-scratch/#3-building-and-training-the-model","title":"3. Building and Training the Model","text":"<pre><code>def hinge_loss(w, b, X, y, C):\n    distances = 1 - y * (np.dot(X, w) + b)\n    distances = np.maximum(0, distances)\n    loss = 0.5 * np.dot(w, w) + C * np.sum(distances)\n    return loss\n</code></pre> <p>This function calculates the hinge loss with L2 regularization. The term <code>np.dot(w, w)</code> penalizes large weights, encouraging a larger margin.</p> <pre><code>def compute_gradients(w, b, X, y, C):\n    distances = 1 - y * (np.dot(X, w) + b)\n    dw = np.zeros_like(w)\n    db = 0\n\n    for i, d in enumerate(distances):\n        if d &gt; 0:\n            dw += -C * y[i] * X[i]\n            db += -C * y[i]\n        else:\n            continue\n\n    dw += w\n    return dw, db\n</code></pre> <p>We compute gradients of the hinge loss for gradient descent updates. When a sample violates the margin, it contributes to the loss.</p> <pre><code>def train_svm(X, y, C=1.0, learning_rate=0.001, epochs=1000):\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0\n\n    for epoch in range(epochs):\n        dw, db = compute_gradients(w, b, X, y, C)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n        if epoch % 100 == 0:\n            loss = hinge_loss(w, b, X, y, C)\n            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n\n    return w, b\n</code></pre> <p>We use simple gradient descent to minimize the loss. Every 100 epochs we print the current loss to monitor training progress.</p> <pre><code>w, b = train_svm(X_train, y_train, C=1.0, learning_rate=0.001, epochs=1000)\n</code></pre> <pre>\nEpoch 0: Loss = 127.0344\nEpoch 100: Loss = 39.6987\nEpoch 200: Loss = 38.9753\nEpoch 300: Loss = 38.9016\nEpoch 400: Loss = 38.8979\nEpoch 500: Loss = 38.8956\nEpoch 600: Loss = 38.8929\nEpoch 700: Loss = 38.8929\nEpoch 800: Loss = 38.8923\nEpoch 900: Loss = 38.8924\n</pre> <p>We train the model using the training data.</p>"},{"location":"blog/implementing-svm-from-scratch/#4-making-predictions-and-evaluation","title":"4. Making Predictions and Evaluation","text":"<pre><code>def predict(X, w, b):\n    return np.sign(np.dot(X, w) + b)\n\ny_pred = predict(X_test, w, b)\naccuracy = np.mean(y_pred == y_test)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n</code></pre> <p>Test Accuracy: 97.00%</p> <p>We use the learned parameters to make predictions on the test set and evaluate accuracy.</p>"},{"location":"blog/implementing-svm-from-scratch/#5-visualization-decision-boundary","title":"5. Visualization Decision Boundary","text":"<pre><code>plt.figure(figsize=(8,6))\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='bwr', alpha=0.7)\n\nx0 = np.linspace(X_test[:, 0].min(), X_test[:, 0].max(), 100)\nx1 = -(w[0] * x0 + b) / w[1]\n\nmargin = 1 / np.linalg.norm(w)\nx1_plus = x1 + margin\nx1_minus = x1 - margin\n\nplt.plot(x0, x1, 'k--', label='Decision boundary')\nplt.plot(x0, x1_plus, 'k:', label='Margin')\nplt.plot(x0, x1_minus, 'k:', label='Margin')\nplt.legend()\nplt.grid()\nplt.title('SVM Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n</code></pre> <p>We plot the decision boundary and margins. Support vectors are not explicitly marked, but they lie closest to the boundary.</p>"},{"location":"blog/implementing-svm-from-scratch/#implementation-using-scikit-learn","title":"Implementation using Scikit-learn","text":"<pre><code>from sklearn.svm import SVC\n\nclf = SVC(kernel='linear', C=1.0)\nclf.fit(X_train, y_train)\nprint(f\"Test Accuracy: {clf.score(X_test, y_test) * 100:.2f}%\")\n</code></pre> <p>Test Accuracy: 97.00%</p> <pre><code>from sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n    clf,\n    X_test,\n    response_method=\"predict\",\n    xlabel=\"Feature 1\",\n    ylabel=\"Feature 2\",\n    cmap=plt.cm.coolwarm,\n)\n\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\", alpha=0.7)\ndisp.ax_.set_xticks([])\ndisp.ax_.set_yticks([])\nplt.show()\n</code></pre> <p></p>"},{"location":"blog/implementing-svm-from-scratch/#conclusion","title":"Conclusion","text":"<p>Support Vector Machines are elegant, powerful classifiers rooted in convex optimization. While libraries like scikit-learn make it easy to use SVMs in practice, building one from scratch helps in deeply understanding how margin-based classifiers operate under the hood. Through a step-by-step approach using NumPy and pandas, we demystified the algorithm and gave you a peek into its mathematical foundation.</p>"},{"location":"blog/introduction-to-machine-learning/","title":"Introduction to Machine Learning","text":"<p>Have you ever wondered how Netflix knows exactly what show you might want to watch next? Or how your email automatically filters spam messages? Or maybe how your phone recognizes your face? All of these technological wonders are powered by machine learning.</p> AI Summary <p>Machine learning is a field where computers learn from data rather than following explicit programming. The text outlines three main types: supervised learning (using labeled examples), unsupervised learning (finding patterns in unlabeled data), and reinforcement learning (learning through rewards and penalties). It explains various algorithms within each type and highlights how machine learning is used in everyday applications like recommendation systems, spam filters, and facial recognition.</p>"},{"location":"blog/introduction-to-machine-learning/#what-is-machine-learning","title":"What is Machine Learning?","text":"<p>Machine learning is a fascinating field where we teach computers to learn from data rather than explicitly programming them with step-by-step instructions. Instead of writing detailed rules for every situation, we let the computer discover patterns and make decisions based on examples.</p> <p>Think of it like teaching a child. You don't give them explicit rules for identifying every dog they might encounter. Instead, you show them many examples of dogs, and eventually, they learn to recognize dogs of all shapes and sizes. Machine learning works similarly \u2013 we feed computers lots of examples, and they learn to recognize patterns and make predictions.</p>"},{"location":"blog/introduction-to-machine-learning/#types-of-machine-learning","title":"Types of Machine Learning","text":""},{"location":"blog/introduction-to-machine-learning/#1-supervised-learning","title":"1. Supervised Learning","text":"<p>Supervised learning is like learning with a teacher. We provide the computer with labeled examples \u2013 input data paired with the correct output \u2013 and it learns to predict the output for new, unseen inputs.</p> <p>Imagine you're teaching a child to identify fruits. You show them apples, oranges, and bananas while naming each one. After seeing enough examples, the child can identify new fruits they haven't seen before. That's supervised learning in action!</p> <p>Some common supervised learning algorithms include:</p> <ul> <li> <p>Linear Regression: This algorithm helps us predict continuous values, like house prices based on features such as square footage, number of bedrooms, and location. It's like drawing a straight line through data points that best fits the relationship between inputs and outputs.</p> </li> <li> <p>Logistic Regression: Despite its name, this is actually a classification algorithm, not a regression algorithm! It predicts binary outcomes like \"yes/no\" or \"spam/not spam.\" Think of it as answering questions like \"Will this customer buy our product?\" or \"Is this email spam?\" It works by calculating the probability that an input belongs to a particular class and making a decision based on that probability. It's especially popular for its simplicity and interpretability.</p> </li> <li> <p>Decision Trees: These algorithms make decisions by creating a tree-like model of decisions and their possible consequences. It's similar to a flowchart where each node represents a feature, each branch represents a decision rule, and each leaf represents an outcome.</p> </li> <li> <p>Support Vector Machines (SVMs): These algorithms find the best boundary that separates different classes of data. Imagine drawing a line (or a hyperplane in higher dimensions) that maximizes the distance between the closest points of different classes.</p> </li> </ul>"},{"location":"blog/introduction-to-machine-learning/#2-unsupervised-learning","title":"2. Unsupervised Learning","text":"<p>Unsupervised learning is like exploring without a teacher. We provide the computer with unlabeled data, and it discovers patterns, structures, or relationships on its own.</p> <p>Think about how you might sort your laundry without someone telling you how. You naturally group similar items together \u2013 all the whites in one pile, dark colors in another, and so on. That's unsupervised learning!</p> <p>Some common unsupervised learning algorithms include:</p> <ul> <li> <p>K-means Clustering: This algorithm groups similar data points together. It's like sorting marbles by color without being told what the colors are \u2013 you simply group similar-looking marbles together.</p> </li> <li> <p>Principal Component Analysis (PCA): This technique reduces the dimensionality of data while preserving as much information as possible. It's like summarizing a long story in a few key points \u2013 you lose some details but keep the essential information.</p> </li> <li> <p>Hierarchical Clustering: This creates a tree of clusters, where similar data points are grouped together at different levels. It's like organizing animals into groups \u2013 mammals, birds, reptiles \u2013 and then further dividing each group into more specific categories.</p> </li> </ul>"},{"location":"blog/introduction-to-machine-learning/#3-reinforcement-learning","title":"3. Reinforcement Learning","text":"<p>Reinforcement learning is like learning through trial and error with rewards and penalties. The computer (or agent) learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties.</p> <p>Imagine teaching a dog new tricks. You give it treats when it does something right and withhold treats when it does something wrong. Over time, the dog learns which behaviors lead to treats. That's reinforcement learning!</p> <p>Some common reinforcement learning algorithms include:</p> <ul> <li> <p>Q-Learning: This algorithm learns the value of taking a particular action in a particular state. It's like learning which route to take to work based on traffic conditions \u2013 you learn which roads are best under different circumstances.</p> </li> <li> <p>Deep Q Networks (DQN): This combines Q-learning with neural networks to handle more complex scenarios. It's what powers many game-playing AI systems that can beat human champions at chess, Go, and video games.</p> </li> <li> <p>Policy Gradient Methods: These algorithms directly learn the best policy (strategy) for achieving goals. Instead of learning values of states and actions, they learn which actions to take in different situations.</p> </li> </ul>"},{"location":"blog/introduction-to-machine-learning/#machine-learning-in-practice","title":"Machine Learning in Practice","text":"<p>Machine learning is all around us, shaping our daily experiences in ways we might not even realize. When you use a voice assistant like Siri or Alexa, machine learning algorithms are processing your speech. When you get personalized recommendations on shopping websites, that's machine learning predicting what you might like based on your past behavior and the behavior of similar users.</p> <p>Even the photos you take on your smartphone benefit from machine learning \u2013 features like portrait mode and night sight use complex algorithms to enhance your images.</p> <p>The beauty of machine learning is that it can find patterns in data that humans might miss. It can process vast amounts of information quickly and make predictions or decisions based on that information. As we continue to generate more and more data, machine learning becomes increasingly powerful and valuable.</p> <p>So next time your phone suggests the perfect song for your mood or your email filters out spam before you even see it, remember \u2013 that's machine learning at work!</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/tutorial/","title":"Tutorial","text":""},{"location":"blog/page/2/","title":"All Blogs","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""},{"location":"blog/category/tutorial/page/2/","title":"Tutorial","text":""}]}